# Архитектура проекта

**Author:** Погосян Артем Артурович (Pogosian Artem)  
**VK:** https://vk.com/iamartempn

## Общая архитектура

Проект построен по принципу микросервисной архитектуры с разделением на три основных компонента:

```
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│  Frontend   │─────▶│   Backend   │─────▶│     LLM     │
│   (React)   │◀─────│  (FastAPI)  │◀─────│   (Ollama)  │
└─────────────┘      └─────────────┘      └─────────────┘
                            │
                            ▼
                     ┌─────────────┐
                     │  Database   │
                     │  (SQLite)   │
                     └─────────────┘
```

## Компоненты

### 1. Frontend (React + TypeScript)

**Технологии:**
- React 18 с TypeScript
- Vite для сборки
- Axios для HTTP-запросов

**Структура:**
- `App.tsx` — главный компонент
- `components/Chat.tsx` — компонент чата
- `components/Sidebar.tsx` — боковая панель с режимами
- `components/QuickActions.tsx` — быстрые действия
- `api/client.ts` — API клиент

**Особенности:**
- Адаптивный дизайн
- Модальные окна для быстрых сценариев
- Обработка ошибок и состояний загрузки

### 2. Backend (FastAPI)

**Технологии:**
- FastAPI для REST API
- SQLAlchemy для работы с БД
- Pydantic для валидации
- HTTPX для асинхронных запросов к LLM

**Структура:**
- `main.py` — точка входа, настройка приложения
- `config.py` — конфигурация через Pydantic Settings
- `db.py` — подключение к БД, сессии
- `models.py` — ORM модели (Conversation, Message)
- `schemas.py` — Pydantic схемы для валидации
- `llm_client.py` — клиент для работы с Ollama
- `routers/` — эндпоинты API

**API Endpoints:**
- `/api/chat` — основной чат
- `/api/usecases/*` — быстрые сценарии
- `/api/health` — health check

### 3. LLM (Ollama)

**Технологии:**
- Ollama — локальный сервер для LLM
- Модель: llama3 (или другая open-source)

**Особенности:**
- Работает в отдельном контейнере
- HTTP API для взаимодействия
- Поддержка различных моделей

### 4. Database (SQLite)

**Технологии:**
- SQLite для простоты (MVP)
- SQLAlchemy ORM

**Модели:**
- `Conversation` — сессии диалогов
- `Message` — сообщения пользователя и ассистента

## Поток данных

### Чат с ИИ

1. Пользователь отправляет сообщение через UI
2. Frontend отправляет `POST /api/chat` с `message`, `mode`, `conversation_id`
3. Backend:
   - Создаёт/находит Conversation
   - Сохраняет сообщение пользователя в БД
   - Получает историю диалога
   - Формирует system prompt на основе `mode`
   - Отправляет запрос к Ollama через `llm_client`
4. Ollama генерирует ответ
5. Backend:
   - Сохраняет ответ ассистента в БД
   - Возвращает ответ и историю Frontend
6. Frontend отображает ответ пользователю

### Быстрые сценарии

1. Пользователь выбирает быстрый сценарий (например, "Составить договор")
2. Открывается модальное окно с формой
3. Пользователь заполняет форму
4. Frontend отправляет запрос на соответствующий эндпоинт `/api/usecases/*`
5. Backend:
   - Формирует специализированный промпт
   - Вызывает LLM с нужным режимом
   - Обрабатывает ответ (парсинг, структурирование)
   - Возвращает результат
6. Frontend показывает результат в модальном окне

## Режимы работы (Modes)

Система использует универсальный чат с автоматическим определением контекста, но поддерживает специализированные системные промпты для различных типов задач:

1. **general** — общий бизнес-ассистент (основной режим)
2. **legal** — юридический помощник (с дисклеймерами)
3. **marketing** — маркетинг и контент
4. **finance** — финансы и операции (с дисклеймерами)
5. **summary** — резюмирование текстов
6. **company** — анализ компаний и создание карточек
7. **taxes** — налоговые консультации

Промпты определены в `llm_client.py` в методе `_get_system_prompt()`. Система автоматически выбирает подходящий промпт на основе контекста запроса.

## Безопасность

- CORS настроен для разрешённых источников
- Валидация входных данных через Pydantic
- SQL-инъекции предотвращены через ORM
- Дисклеймеры в юридических и финансовых режимах

## Масштабируемость

### Текущая архитектура (MVP)
- SQLite для простоты
- Один экземпляр каждого сервиса
- Локальная LLM

### Возможные улучшения
- Замена SQLite на PostgreSQL для production
- Горизонтальное масштабирование backend (несколько инстансов)
- Кэширование ответов LLM
- Очереди задач для длительных операций (Celery, Redis)
- Мониторинг и логирование (Prometheus, Grafana)
- Использование более мощных LLM через API (при необходимости)

## Интеграция в экосистему банка

### Варианты интеграции:

1. **Интернет-банк / Мобильный банк**
   - Встраивание как iframe или отдельный раздел
   - Использование банковского API для доступа к финансовым данным
   - Авторизация через банковскую систему

2. **Нейроофис банка**
   - Интеграция как дополнительный ассистент
   - Связь с банковскими продуктами (кредиты, счета, аналитика)
   - Единая система авторизации

3. **Мессенджеры**
   - Telegram-бот с тем же функционалом
   - Интеграция в корпоративные чаты (Slack, Teams)
   - Webhook для обработки сообщений

### Технические требования для интеграции:

- REST API готов для интеграции
- Возможность добавления middleware для авторизации
- Модульная архитектура позволяет легко расширять функционал
- Поддержка webhooks для внешних систем

## Развёртывание

### Docker Compose (текущее решение)
- Все сервисы в контейнерах
- Простое развёртывание одной командой
- Изоляция окружений

### Production развёртывание
- Kubernetes для оркестрации
- Отдельные сервисы для каждого компонента
- Load balancer для frontend и backend
- Persistent volumes для данных
- Мониторинг и логирование

## Заключение

Архитектура спроектирована с учётом:
- Простоты разработки и развёртывания
- Возможности масштабирования
- Готовности к интеграции в банковскую экосистему
- Использования только open-source технологий

