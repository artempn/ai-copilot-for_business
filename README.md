**Author:** Погосян Артем Артурович (Pogosian Artem)  
**VK:** https://vk.com/iamartempn

## Описание проекта

**AI Copilot для Малого Бизнеса** — интеллектуальный помощник для владельцев малого бизнеса, помогающий решать повседневные задачи с использованием локальных open-source языковых моделей.

### Основные возможности:

- **Универсальный чат** — задавайте любые вопросы о бизнесе, система автоматически определит контекст
- **Юридические вопросы** — составление договоров, формулировки, базовый анализ рисков
- **Маркетинг** — создание постов для соцсетей, промоакции, описания товаров и услуг
- **Финансы** — анализ продаж/расходов, рекомендации по управлению денежным потоком
- **Обработка текстов** — резюмирование переписок, создание чек-листов, извлечение задач
- **Карточка компании** — создание структурированной карточки на основе ИНН, адреса и других данных
- **Консультации по налогам** — ответы на вопросы о налоговых режимах, расчётах и отчётности

## Технологический стек

### Backend
- **Python 3.11+**
- **FastAPI** — современный веб-фреймворк
- **SQLAlchemy** — ORM для работы с БД
- **SQLite** — база данных (для MVP)
- **HTTPX** — асинхронные HTTP-запросы к LLM

### Frontend
- **React 18** — библиотека для UI
- **TypeScript** — типизация
- **Vite** — сборщик и dev-сервер
- **Axios** — HTTP-клиент

### LLM
- **Ollama** — локальный сервер для запуска LLM
- **llama3.2:3b** — оптимизированная языковая модель (быстрая и эффективная)
- Поддержка других моделей: mistral, phi3, qwen и др.

### Инфраструктура
- **Docker** + **docker-compose** — контейнеризация
- **Nginx** — веб-сервер для frontend (production)

## Структура проекта

```
Case_from_Alpha/
├── backend/
│   ├── app/
│   │   ├── __init__.py
│   │   ├── main.py              # FastAPI приложение
│   │   ├── config.py             # Конфигурация
│   │   ├── db.py                 # Подключение к БД
│   │   ├── models.py             # ORM модели
│   │   ├── schemas.py            # Pydantic схемы
│   │   ├── llm_client.py         # Клиент для LLM
│   │   ├── deps.py               # Зависимости
│   │   └── routers/
│   │       ├── chat.py           # Чат эндпоинт
│   │       ├── usecases.py       # Быстрые сценарии
│   │       └── health.py         # Health check
│   ├── requirements.txt
│   └── Dockerfile
├── frontend/
│   ├── src/
│   │   ├── App.tsx
│   │   ├── main.tsx
│   │   ├── api/
│   │   │   └── client.ts         # API клиент
│   │   └── components/
│   │       ├── Chat.tsx
│   │       ├── Sidebar.tsx
│   │       └── QuickActions.tsx
│   ├── package.json
│   ├── vite.config.ts
│   └── Dockerfile
├── docker-compose.yml
├── README.md
└── docs/
    └── ARCHITECTURE.md
```

## Быстрый старт

### Предварительные требования

- Docker и Docker Compose установлены
- Минимум 8GB RAM (для запуска LLM)
- Свободное место на диске (для модели LLM, ~4-8GB)

### Запуск проекта

1. **Клонируйте репозиторий:**
   ```bash
   git clone <repo-url>
   cd Case_from_Alpha
   ```

2. **Запустите все сервисы:**
   ```bash
   docker compose up --build
   ```

   При первом запуске это может занять время, так как:
   - Собираются образы backend и frontend
   - Скачивается образ Ollama
   - Скачивается модель LLM (при первом запросе)

3. **Подготовка модели LLM (если нужно):**
   
   После запуска контейнера `llm`, выполните:
   ```bash
   docker exec -it copilot-llm ollama pull llama3
   ```
   
   Или используйте другую модель:
   ```bash
   docker exec -it copilot-llm ollama pull mistral
   ```
   
   Затем обновите `LLM_MODEL` в `docker-compose.yml` или переменных окружения.

4. **Проверка работы:**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8000
   - API Docs: http://localhost:8000/docs
   - Ollama: http://localhost:11434

### Остановка

```bash
docker compose down
```

Для полной очистки (включая volumes):
```bash
docker compose down -v
```

## Демо-сценарии

### 1. Универсальный чат
- Откройте http://localhost:3000
- Задайте любой вопрос о бизнесе, например:
  - "Как выбрать налоговый режим для ИП?"
  - "Составь договор аренды офиса"
  - "Создай пост для Instagram о новой услуге"
  - "Проанализируй мои продажи за месяц"

### 2. Быстрые действия

**Составить договор:**
- Нажмите кнопку "Составить договор"
- Заполните форму (тип договора, стороны, предмет, сумма)
- Получите готовый черновик договора

**Создать промо-пост:**
- Нажмите "Создать промо-пост"
- Укажите описание бизнеса, цель промоакции, платформу
- Получите несколько вариантов постов

**Финансовый отчёт:**
- Нажмите "Финансовый отчёт"
- Введите данные по продажам и расходам
- Получите анализ и рекомендации

**Резюме текста:**
- Нажмите "Резюме текста"
- Вставьте длинный текст
- Получите структурированное резюме с задачами

**Карточка компании:**
- Нажмите "Карточка компании"
- Введите ИНН, название, адрес
- Получите структурированную карточку с рекомендациями

**Консультация по налогам:**
- Нажмите "Консультация по налогам"
- Задайте вопрос о налогах
- Укажите тип бизнеса и налоговый режим (опционально)
- Получите подробный ответ с расчётами

## API Endpoints

### Chat
- `POST /api/chat` — основной чат с ИИ

### Use Cases
- `POST /api/usecases/legal-contract` — генерация договора
- `POST /api/usecases/marketing-post` — создание промо-поста
- `POST /api/usecases/finance-report` — финансовый анализ
- `POST /api/usecases/summary` — резюмирование текста
- `POST /api/usecases/company-card` — создание карточки компании
- `POST /api/usecases/tax-consultation` — консультация по налогам

### Health
- `GET /api/health` — проверка статуса сервиса

Подробная документация доступна по адресу: http://localhost:8000/docs

## Архитектура

Подробное описание архитектуры см. в [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)

### Краткое описание

```
UI (React) → Backend (FastAPI) → LLM (Ollama) → Backend → UI
                ↓
            Database (SQLite)
```

### Интеграция в экосистему банка

Решение может быть интегрировано в:

1. **Интернет-банк / Мобильный банк**
   - Как отдельный раздел "Бизнес-помощник"
   - Через API банка для доступа к финансовым данным клиента

2. **Нейроофис банка**
   - Как дополнительный ассистент с фокусом на малый бизнес
   - Интеграция с банковскими продуктами и услугами

3. **Мессенджеры**
   - Telegram-бот
   - Интеграция в корпоративные чаты

## Ограничения и дисклеймеры

⚠️ **Важно:**

- **Система находится в стадии разработки и может содержать неточности.** Все ответы генерируются локальной языковой моделью и могут быть неполными или содержать ошибки. Всегда проверяйте критически важную информацию из независимых источников.

- Модель **не даёт юридических гарантий**. Все юридические рекомендации носят справочный характер. Для критичных вопросов необходимо обратиться к квалифицированному юристу.

- Модель **не даёт финансовых гарантий** или инвестиционных советов. Все финансовые рекомендации — общие и не учитывают индивидуальные обстоятельства. Для серьёзных финансовых решений обратитесь к финансовому консультанту.

- **Неточности в ответах связаны с несовершенством системы** и ограничениями локальных языковых моделей. Система постоянно улучшается, но не может гарантировать 100% точность ответов.

- Модель работает локально и требует достаточных вычислительных ресурсов (минимум 8GB RAM).

- Всегда используйте ответы системы как отправную точку для дальнейшего исследования, а не как окончательное решение.

## Разработка

### Локальная разработка (без Docker)

#### Backend
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

#### Frontend
```bash
cd frontend
npm install
npm run dev
```

#### LLM
Установите Ollama локально: https://ollama.ai

Запустите:
```bash
ollama serve
ollama pull llama3
```

### Переменные окружения

Создайте `.env` файл в корне проекта (опционально):

```env
LLM_MODEL=llama3
LLM_BASE_URL=http://localhost:11434
DATABASE_URL=sqlite:///./copilot.db
SAVE_HISTORY=true
```

## Troubleshooting

### LLM не отвечает
- Проверьте, что Ollama запущен: `docker ps | grep llm`
- Проверьте логи: `docker logs copilot-llm`
- Убедитесь, что модель загружена: `docker exec -it copilot-llm ollama list`

### Frontend не подключается к Backend
- Проверьте переменную `VITE_API_URL` в `docker-compose.yml`
- Убедитесь, что backend доступен: http://localhost:8000/api/health

### Ошибки при сборке
- Убедитесь, что Docker имеет достаточно памяти (минимум 8GB)
- Проверьте логи: `docker compose logs`

## Лицензия

Проект создан для кейса.

## Контакты

**Автор:** Погосян Артем Артурович (Pogosian Artem)  
**VK:** https://vk.com/iamartempn

